services:
  # Main API Service
  api:
    build:
      context: ./api
      dockerfile: Dockerfile
    ports:
      - "0.0.0.0:8000:8000"
    environment:
      - LLM_PROVIDER=${LLM_PROVIDER:-ollama}
      - LLM_MODEL=${LLM_MODEL:-mistral:7b}
      - OLLAMA_HOST=http://ollama:11434
      - DATABASE_URL=postgresql://bible:bible123@postgres:5432/bibledb
      - EMBEDDING_MODEL=${EMBEDDING_MODEL:-nomic-embed-text}
    depends_on:
      postgres:
        condition: service_healthy
      ollama:
        condition: service_started
    volumes:
      - ./api:/app
    command: uvicorn main:app --host 0.0.0.0 --port 8000 --reload

  # Ollama - Self-hosted LLM
  ollama:
    image: ollama/ollama:latest
    ports:
      - "0.0.0.0:11434:11434"
    environment:
      - LLM_MODEL=${LLM_MODEL:-mistral:7b}
      - EMBEDDING_MODEL=${EMBEDDING_MODEL:-nomic-embed-text}
    volumes:
      - ollama_data:/root/.ollama
      - ./scripts/init-ollama.sh:/init-ollama.sh
    entrypoint: ["/bin/bash", "/init-ollama.sh"]
    # For CPU-only, comment out or remove the deploy section below
    # Uncomment for GPU support (requires nvidia-docker)
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]

  # PostgreSQL with pgvector for semantic search
  postgres:
    image: pgvector/pgvector:pg16
    ports:
      - "0.0.0.0:5432:5432"
    environment:
      - POSTGRES_USER=bible
      - POSTGRES_PASSWORD=bible123
      - POSTGRES_DB=bibledb
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./scripts/init.sql:/docker-entrypoint-initdb.d/init.sql
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U bible -d bibledb"]
      interval: 5s
      timeout: 5s
      retries: 5

  # Frontend (Next.js)
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    ports:
      - "0.0.0.0:3000:3000"
    environment:
      # In production with reverse proxy, API is on same domain
      - NEXT_PUBLIC_API_URL=${NEXT_PUBLIC_API_URL:-http://localhost:8000}
    volumes:
      - ./frontend:/app
      - /app/node_modules
    depends_on:
      - api

volumes:
  ollama_data:
  postgres_data:
